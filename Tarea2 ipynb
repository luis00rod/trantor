{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319b4063",
   "metadata": {},
   "source": [
    "### Libraries to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bddd5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730b68f",
   "metadata": {},
   "source": [
    "### The following classes are designed to be used in the tasks two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2720b8c",
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.instance_data = self.read_instances()\n",
    "        self.variables_df = self.read_variables()\n",
    "        self.instances_list = self.transform_instance()\n",
    "        self.flat_instance_df = self.concatenation()\n",
    "        self.corpus = self.flat_instance_df['element'].map(lambda row: [row]).tolist()\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_instances() -> list:\n",
    "\n",
    "        with open(\"data.pickle\", \"rb\") as file:\n",
    "            instances = pickle.load(file)\n",
    "        \n",
    "        return instances\n",
    "\n",
    "    @staticmethod\n",
    "    def read_variables() -> pd.DataFrame:\n",
    "\n",
    "        with open(\"variables.pickle\", \"rb\") as file:\n",
    "            variables = pickle.load(file)\n",
    "\n",
    "        return pd.DataFrame(variables, columns=('var11', 'var12', 'var13', 'var14', 'var15'))\n",
    "\n",
    "    def transform_instance(self) -> list:\n",
    "        instances_list = []\n",
    "        for instance in range(0, len(self.instance_data[:])):\n",
    "\n",
    "            elements = []\n",
    "\n",
    "            for element in range(len(self.instance_data[instance][0])):\n",
    "                elements.append(\"ele\" + str(element))\n",
    "            total_df = pd.DataFrame(self.instance_data[instance], columns=elements)\n",
    "            instances_list.append(total_df)\n",
    "\n",
    "        return instances_list\n",
    "\n",
    "    def concatenation(self) -> pd.DataFrame:\n",
    "        instance = []\n",
    "        for instances_df in range(0, len(self.instances_list)):\n",
    "            for column in self.instances_list[instances_df].columns:\n",
    "                instance.append(pd.concat([self.transform_instance()[instances_df][column], self.variables_df], axis=1)\n",
    "                                .rename(columns={column: 'element', 'var1': 'var1',\n",
    "                                                 'var2': 'var2', 'var3': 'var3', 'var4': 'var4', 'var5': 'var5'})\n",
    "                                )\n",
    "\n",
    "        instance = pd.concat(instance)\n",
    "        final_instance = instance[instance[\"element\"].str.contains(\"nan\") == False].reset_index().drop(\"index\", axis=1)\n",
    "        \n",
    "        \n",
    "        return final_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5bcdce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_trantor = Predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beade91c",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "\n",
    "2.1) Perform the necessary transformations of the data so that it consists of six columns. The first column would consist of each element of each group in the data, and the second coulmn would contain its corresponding variables to be predicted. Following the example two cells above, one line in the dataset would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4c0fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>element</th>\n",
       "      <th>var11</th>\n",
       "      <th>var12</th>\n",
       "      <th>var13</th>\n",
       "      <th>var14</th>\n",
       "      <th>var15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mercadillo</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Primark</td>\n",
       "      <td>99.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donde puedo</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mercados ambulantes</td>\n",
       "      <td>75.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No compro</td>\n",
       "      <td>99.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34781</th>\n",
       "      <td>Snooker</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34782</th>\n",
       "      <td>Zumba</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34783</th>\n",
       "      <td>Bouzuki</td>\n",
       "      <td>70.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34784</th>\n",
       "      <td>Timple</td>\n",
       "      <td>99.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34785</th>\n",
       "      <td>Laúd y bandurria</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34786 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   element  var11  var12  var13  var14  var15\n",
       "0              Mercadillo    90.0   30.0   65.0   20.0    5.0\n",
       "1                  Primark   99.0   25.0   65.0   80.0   25.0\n",
       "2              Donde puedo   97.0    1.0   20.0   20.0   50.0\n",
       "3      Mercados ambulantes   75.0   65.0   85.0   40.0   90.0\n",
       "4                No compro   99.0   10.0   96.0   40.0    1.0\n",
       "...                    ...    ...    ...    ...    ...    ...\n",
       "34781             Snooker    99.0    1.0   35.0   10.0    5.0\n",
       "34782                Zumba   15.0   20.0   80.0   15.0   30.0\n",
       "34783              Bouzuki   70.0   15.0   85.0   10.0   20.0\n",
       "34784               Timple   99.0   20.0   70.0    5.0   70.0\n",
       "34785     Laúd y bandurria   85.0   60.0   10.0   60.0    5.0\n",
       "\n",
       "[34786 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_trantor.flat_instance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e475a892",
   "metadata": {},
   "source": [
    "2.2 Transform the elements to a numeric representation using word embeddings. We recommend the gensim library, but feel free to use any other. Transform the dataset again, this time appending the values obtained from the WE to the values to be predicted. This way, the dataset will now have n+5 columns, being n the number of dimension of the WE chosen for the transformation. Note that many items will consist of multiple words, with which many WE are not compatible. To that end, figure out away of \"combining\" the multiple words of one element (e.g., the mean of the different values, or any other approach you come up with)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c4019",
   "metadata": {},
   "source": [
    "#### We are using the simplest model of word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdd70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "data_for_training = list(tagged_document(to_trantor.corpus))\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
    "model.build_vocab(data_for_training)\n",
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91225b0",
   "metadata": {},
   "source": [
    "### Creation of the numeric representation of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d33180",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_trantor.flat_instance_df[\"infer_vector\"] = to_trantor.flat_instance_df[\"element\"].\\\n",
    "                                                apply(lambda x: model.infer_vector([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b954c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var11</th>\n",
       "      <th>var12</th>\n",
       "      <th>var13</th>\n",
       "      <th>var14</th>\n",
       "      <th>var15</th>\n",
       "      <th>infer_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.008179409, 0.008700813, 0.00847017, -0.0043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>[0.00042638002, 0.0060320054, -0.0084115425, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>[0.007742472, -0.006338848, 0.0044196956, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>[0.004653619, -0.007879323, 0.005447209, 0.008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.0032491789, 0.0061508478, 0.0057833553, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34781</th>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[-0.0075379154, -0.0043836045, 0.011766913, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34782</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>[-0.0083402125, -0.006200726, -0.008781448, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34783</th>\n",
       "      <td>70.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[-0.007012859, -0.010679213, -0.01057968, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34784</th>\n",
       "      <td>99.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>[0.011038308, 0.009843322, 0.00797001, -0.0017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34785</th>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[-0.008723842, -0.008314538, -0.010405777, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34786 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       var11  var12  var13  var14  var15  \\\n",
       "0       90.0   30.0   65.0   20.0    5.0   \n",
       "1       99.0   25.0   65.0   80.0   25.0   \n",
       "2       97.0    1.0   20.0   20.0   50.0   \n",
       "3       75.0   65.0   85.0   40.0   90.0   \n",
       "4       99.0   10.0   96.0   40.0    1.0   \n",
       "...      ...    ...    ...    ...    ...   \n",
       "34781   99.0    1.0   35.0   10.0    5.0   \n",
       "34782   15.0   20.0   80.0   15.0   30.0   \n",
       "34783   70.0   15.0   85.0   10.0   20.0   \n",
       "34784   99.0   20.0   70.0    5.0   70.0   \n",
       "34785   85.0   60.0   10.0   60.0    5.0   \n",
       "\n",
       "                                            infer_vector  \n",
       "0      [0.008179409, 0.008700813, 0.00847017, -0.0043...  \n",
       "1      [0.00042638002, 0.0060320054, -0.0084115425, -...  \n",
       "2      [0.007742472, -0.006338848, 0.0044196956, 0.00...  \n",
       "3      [0.004653619, -0.007879323, 0.005447209, 0.008...  \n",
       "4      [-0.0032491789, 0.0061508478, 0.0057833553, 0....  \n",
       "...                                                  ...  \n",
       "34781  [-0.0075379154, -0.0043836045, 0.011766913, 0....  \n",
       "34782  [-0.0083402125, -0.006200726, -0.008781448, 0....  \n",
       "34783  [-0.007012859, -0.010679213, -0.01057968, 0.00...  \n",
       "34784  [0.011038308, 0.009843322, 0.00797001, -0.0017...  \n",
       "34785  [-0.008723842, -0.008314538, -0.010405777, -0....  \n",
       "\n",
       "[34786 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_trantor.flat_instance_df.drop([\"element\"], axis=1, inplace=True)\n",
    "to_trantor.flat_instance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6ae80",
   "metadata": {},
   "source": [
    "2.3.1: Choose a set of \"pivot\" values. These are vectors of the same dimension as the one of the chosen WE. The values of the pivots are arbitrary. You can choose random values, zeros, ones, twos, ..., even an item which has been transformed into its vectorized form in the previous step can be used as a pivot. You have to choose 10 pivots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf07999",
   "metadata": {},
   "source": [
    "### Pivots choosen with psudo-random numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58dec5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_values = pd.Series([np.random.rand(40), np.random.rand(40),np.random.rand(40),\\\n",
    "               np.random.rand(40),np.random.rand(40),np.random.rand(40),\\\n",
    "               np.random.rand(40),np.random.rand(40),np.random.rand(40),np.random.rand(40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200811f0",
   "metadata": {},
   "source": [
    "2.3.2: Next, you will have to compute a distance (e.g., MSE or any other that you may find more suited to this problem) from each vectorized element, to each pivot.\n",
    "\n",
    "#### Following the instruction given, we are computing the MSE between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43387f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_to_pivots(row, pivot_values) -> list:\n",
    "    \n",
    "    infer_variables = []\n",
    "    for pivot_element in range(len(pivot_values)):\n",
    "        \n",
    "        infer_variables.append(mean_squared_error(row, pivot_values.iloc[pivot_element]))\n",
    "        \n",
    "    \n",
    "    return infer_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e22735a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_trantor.flat_instance_df[\"mse_to_pivot\"] = to_trantor.flat_instance_df[\"infer_vector\"].\\\n",
    "                                                apply(lambda x: mse_to_pivots(x, pivot_values))\n",
    "mse_to_pivot = to_trantor.flat_instance_df[\"mse_to_pivot\"].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0efb456c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "      <th>var6</th>\n",
       "      <th>var7</th>\n",
       "      <th>var8</th>\n",
       "      <th>var9</th>\n",
       "      <th>var10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.277783</td>\n",
       "      <td>0.326376</td>\n",
       "      <td>0.441182</td>\n",
       "      <td>0.372562</td>\n",
       "      <td>0.297275</td>\n",
       "      <td>0.319457</td>\n",
       "      <td>0.340955</td>\n",
       "      <td>0.327311</td>\n",
       "      <td>0.336004</td>\n",
       "      <td>0.446072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.273673</td>\n",
       "      <td>0.323362</td>\n",
       "      <td>0.436117</td>\n",
       "      <td>0.370720</td>\n",
       "      <td>0.293343</td>\n",
       "      <td>0.316849</td>\n",
       "      <td>0.336701</td>\n",
       "      <td>0.325443</td>\n",
       "      <td>0.332952</td>\n",
       "      <td>0.442603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.277841</td>\n",
       "      <td>0.326277</td>\n",
       "      <td>0.439304</td>\n",
       "      <td>0.373097</td>\n",
       "      <td>0.296120</td>\n",
       "      <td>0.319040</td>\n",
       "      <td>0.340437</td>\n",
       "      <td>0.327748</td>\n",
       "      <td>0.334825</td>\n",
       "      <td>0.446987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.275610</td>\n",
       "      <td>0.326637</td>\n",
       "      <td>0.441550</td>\n",
       "      <td>0.374201</td>\n",
       "      <td>0.296937</td>\n",
       "      <td>0.318506</td>\n",
       "      <td>0.340583</td>\n",
       "      <td>0.327907</td>\n",
       "      <td>0.336504</td>\n",
       "      <td>0.447544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.277014</td>\n",
       "      <td>0.326817</td>\n",
       "      <td>0.441356</td>\n",
       "      <td>0.372107</td>\n",
       "      <td>0.296488</td>\n",
       "      <td>0.318141</td>\n",
       "      <td>0.341391</td>\n",
       "      <td>0.326297</td>\n",
       "      <td>0.334258</td>\n",
       "      <td>0.445679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34781</th>\n",
       "      <td>0.276415</td>\n",
       "      <td>0.324687</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>0.370576</td>\n",
       "      <td>0.294065</td>\n",
       "      <td>0.317880</td>\n",
       "      <td>0.338912</td>\n",
       "      <td>0.325781</td>\n",
       "      <td>0.333389</td>\n",
       "      <td>0.444859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34782</th>\n",
       "      <td>0.278178</td>\n",
       "      <td>0.327061</td>\n",
       "      <td>0.442757</td>\n",
       "      <td>0.373949</td>\n",
       "      <td>0.296318</td>\n",
       "      <td>0.320729</td>\n",
       "      <td>0.341382</td>\n",
       "      <td>0.327953</td>\n",
       "      <td>0.336902</td>\n",
       "      <td>0.447566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34783</th>\n",
       "      <td>0.277245</td>\n",
       "      <td>0.327361</td>\n",
       "      <td>0.440891</td>\n",
       "      <td>0.374464</td>\n",
       "      <td>0.297082</td>\n",
       "      <td>0.320145</td>\n",
       "      <td>0.341492</td>\n",
       "      <td>0.330135</td>\n",
       "      <td>0.335780</td>\n",
       "      <td>0.447298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34784</th>\n",
       "      <td>0.277999</td>\n",
       "      <td>0.325340</td>\n",
       "      <td>0.439702</td>\n",
       "      <td>0.372921</td>\n",
       "      <td>0.295268</td>\n",
       "      <td>0.317466</td>\n",
       "      <td>0.339503</td>\n",
       "      <td>0.325751</td>\n",
       "      <td>0.334709</td>\n",
       "      <td>0.445737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34785</th>\n",
       "      <td>0.275795</td>\n",
       "      <td>0.325335</td>\n",
       "      <td>0.440185</td>\n",
       "      <td>0.372432</td>\n",
       "      <td>0.294206</td>\n",
       "      <td>0.317931</td>\n",
       "      <td>0.338236</td>\n",
       "      <td>0.327725</td>\n",
       "      <td>0.334642</td>\n",
       "      <td>0.445100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34786 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           var1      var2      var3      var4      var5      var6      var7  \\\n",
       "0      0.277783  0.326376  0.441182  0.372562  0.297275  0.319457  0.340955   \n",
       "1      0.273673  0.323362  0.436117  0.370720  0.293343  0.316849  0.336701   \n",
       "2      0.277841  0.326277  0.439304  0.373097  0.296120  0.319040  0.340437   \n",
       "3      0.275610  0.326637  0.441550  0.374201  0.296937  0.318506  0.340583   \n",
       "4      0.277014  0.326817  0.441356  0.372107  0.296488  0.318141  0.341391   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "34781  0.276415  0.324687  0.438208  0.370576  0.294065  0.317880  0.338912   \n",
       "34782  0.278178  0.327061  0.442757  0.373949  0.296318  0.320729  0.341382   \n",
       "34783  0.277245  0.327361  0.440891  0.374464  0.297082  0.320145  0.341492   \n",
       "34784  0.277999  0.325340  0.439702  0.372921  0.295268  0.317466  0.339503   \n",
       "34785  0.275795  0.325335  0.440185  0.372432  0.294206  0.317931  0.338236   \n",
       "\n",
       "           var8      var9     var10  \n",
       "0      0.327311  0.336004  0.446072  \n",
       "1      0.325443  0.332952  0.442603  \n",
       "2      0.327748  0.334825  0.446987  \n",
       "3      0.327907  0.336504  0.447544  \n",
       "4      0.326297  0.334258  0.445679  \n",
       "...         ...       ...       ...  \n",
       "34781  0.325781  0.333389  0.444859  \n",
       "34782  0.327953  0.336902  0.447566  \n",
       "34783  0.330135  0.335780  0.447298  \n",
       "34784  0.325751  0.334709  0.445737  \n",
       "34785  0.327725  0.334642  0.445100  \n",
       "\n",
       "[34786 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_to_pivot.columns=['var1', 'var2', 'var3', 'var4', 'var5', 'var6', 'var7', 'var8', 'var9', 'var10']\n",
    "mse_to_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd841ae1",
   "metadata": {},
   "source": [
    "2.3.3: These 10 values now represent each element. Append them to the variables, and now you will have a dataset consisting of 15 columns. The first ten will contain the distances from the vectorized version of the elements to each of the topics, and the last five, the variables to be predicted.\n",
    "\n",
    "### Creation of the data to be used in the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e13fd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_trantor.flat_instance_df = pd.concat([mse_to_pivot, to_trantor.flat_instance_df], axis=1).\\\n",
    "                                drop([\"infer_vector\",\"mse_to_pivot\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a4e772b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "      <th>var6</th>\n",
       "      <th>var7</th>\n",
       "      <th>var8</th>\n",
       "      <th>var9</th>\n",
       "      <th>var10</th>\n",
       "      <th>var11</th>\n",
       "      <th>var12</th>\n",
       "      <th>var13</th>\n",
       "      <th>var14</th>\n",
       "      <th>var15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.277783</td>\n",
       "      <td>0.326376</td>\n",
       "      <td>0.441182</td>\n",
       "      <td>0.372562</td>\n",
       "      <td>0.297275</td>\n",
       "      <td>0.319457</td>\n",
       "      <td>0.340955</td>\n",
       "      <td>0.327311</td>\n",
       "      <td>0.336004</td>\n",
       "      <td>0.446072</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.273673</td>\n",
       "      <td>0.323362</td>\n",
       "      <td>0.436117</td>\n",
       "      <td>0.370720</td>\n",
       "      <td>0.293343</td>\n",
       "      <td>0.316849</td>\n",
       "      <td>0.336701</td>\n",
       "      <td>0.325443</td>\n",
       "      <td>0.332952</td>\n",
       "      <td>0.442603</td>\n",
       "      <td>99.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.277841</td>\n",
       "      <td>0.326277</td>\n",
       "      <td>0.439304</td>\n",
       "      <td>0.373097</td>\n",
       "      <td>0.296120</td>\n",
       "      <td>0.319040</td>\n",
       "      <td>0.340437</td>\n",
       "      <td>0.327748</td>\n",
       "      <td>0.334825</td>\n",
       "      <td>0.446987</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.275610</td>\n",
       "      <td>0.326637</td>\n",
       "      <td>0.441550</td>\n",
       "      <td>0.374201</td>\n",
       "      <td>0.296937</td>\n",
       "      <td>0.318506</td>\n",
       "      <td>0.340583</td>\n",
       "      <td>0.327907</td>\n",
       "      <td>0.336504</td>\n",
       "      <td>0.447544</td>\n",
       "      <td>75.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.277014</td>\n",
       "      <td>0.326817</td>\n",
       "      <td>0.441356</td>\n",
       "      <td>0.372107</td>\n",
       "      <td>0.296488</td>\n",
       "      <td>0.318141</td>\n",
       "      <td>0.341391</td>\n",
       "      <td>0.326297</td>\n",
       "      <td>0.334258</td>\n",
       "      <td>0.445679</td>\n",
       "      <td>99.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34781</th>\n",
       "      <td>0.276415</td>\n",
       "      <td>0.324687</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>0.370576</td>\n",
       "      <td>0.294065</td>\n",
       "      <td>0.317880</td>\n",
       "      <td>0.338912</td>\n",
       "      <td>0.325781</td>\n",
       "      <td>0.333389</td>\n",
       "      <td>0.444859</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34782</th>\n",
       "      <td>0.278178</td>\n",
       "      <td>0.327061</td>\n",
       "      <td>0.442757</td>\n",
       "      <td>0.373949</td>\n",
       "      <td>0.296318</td>\n",
       "      <td>0.320729</td>\n",
       "      <td>0.341382</td>\n",
       "      <td>0.327953</td>\n",
       "      <td>0.336902</td>\n",
       "      <td>0.447566</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34783</th>\n",
       "      <td>0.277245</td>\n",
       "      <td>0.327361</td>\n",
       "      <td>0.440891</td>\n",
       "      <td>0.374464</td>\n",
       "      <td>0.297082</td>\n",
       "      <td>0.320145</td>\n",
       "      <td>0.341492</td>\n",
       "      <td>0.330135</td>\n",
       "      <td>0.335780</td>\n",
       "      <td>0.447298</td>\n",
       "      <td>70.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34784</th>\n",
       "      <td>0.277999</td>\n",
       "      <td>0.325340</td>\n",
       "      <td>0.439702</td>\n",
       "      <td>0.372921</td>\n",
       "      <td>0.295268</td>\n",
       "      <td>0.317466</td>\n",
       "      <td>0.339503</td>\n",
       "      <td>0.325751</td>\n",
       "      <td>0.334709</td>\n",
       "      <td>0.445737</td>\n",
       "      <td>99.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34785</th>\n",
       "      <td>0.275795</td>\n",
       "      <td>0.325335</td>\n",
       "      <td>0.440185</td>\n",
       "      <td>0.372432</td>\n",
       "      <td>0.294206</td>\n",
       "      <td>0.317931</td>\n",
       "      <td>0.338236</td>\n",
       "      <td>0.327725</td>\n",
       "      <td>0.334642</td>\n",
       "      <td>0.445100</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34786 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           var1      var2      var3      var4      var5      var6      var7  \\\n",
       "0      0.277783  0.326376  0.441182  0.372562  0.297275  0.319457  0.340955   \n",
       "1      0.273673  0.323362  0.436117  0.370720  0.293343  0.316849  0.336701   \n",
       "2      0.277841  0.326277  0.439304  0.373097  0.296120  0.319040  0.340437   \n",
       "3      0.275610  0.326637  0.441550  0.374201  0.296937  0.318506  0.340583   \n",
       "4      0.277014  0.326817  0.441356  0.372107  0.296488  0.318141  0.341391   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "34781  0.276415  0.324687  0.438208  0.370576  0.294065  0.317880  0.338912   \n",
       "34782  0.278178  0.327061  0.442757  0.373949  0.296318  0.320729  0.341382   \n",
       "34783  0.277245  0.327361  0.440891  0.374464  0.297082  0.320145  0.341492   \n",
       "34784  0.277999  0.325340  0.439702  0.372921  0.295268  0.317466  0.339503   \n",
       "34785  0.275795  0.325335  0.440185  0.372432  0.294206  0.317931  0.338236   \n",
       "\n",
       "           var8      var9     var10  var11  var12  var13  var14  var15  \n",
       "0      0.327311  0.336004  0.446072   90.0   30.0   65.0   20.0    5.0  \n",
       "1      0.325443  0.332952  0.442603   99.0   25.0   65.0   80.0   25.0  \n",
       "2      0.327748  0.334825  0.446987   97.0    1.0   20.0   20.0   50.0  \n",
       "3      0.327907  0.336504  0.447544   75.0   65.0   85.0   40.0   90.0  \n",
       "4      0.326297  0.334258  0.445679   99.0   10.0   96.0   40.0    1.0  \n",
       "...         ...       ...       ...    ...    ...    ...    ...    ...  \n",
       "34781  0.325781  0.333389  0.444859   99.0    1.0   35.0   10.0    5.0  \n",
       "34782  0.327953  0.336902  0.447566   15.0   20.0   80.0   15.0   30.0  \n",
       "34783  0.330135  0.335780  0.447298   70.0   15.0   85.0   10.0   20.0  \n",
       "34784  0.325751  0.334709  0.445737   99.0   20.0   70.0    5.0   70.0  \n",
       "34785  0.327725  0.334642  0.445100   85.0   60.0   10.0   60.0    5.0  \n",
       "\n",
       "[34786 rows x 15 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_trantor.flat_instance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de58b9d",
   "metadata": {},
   "source": [
    "### Normalize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c18fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 10\n",
    "X = to_trantor.flat_instance_df.iloc[:, 0:10]\n",
    "y = to_trantor.flat_instance_df.iloc[:, 13: 14]\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_x.fit(X)\n",
    "scaler_y.fit(y)\n",
    "xscale=scaler_x.transform(X)\n",
    "yscale=scaler_y.transform(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "552f235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41158a5",
   "metadata": {},
   "source": [
    "### Model of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d793147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=10, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f25daad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb65f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "522/522 [==============================] - 2s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.0729 - val_mse: 0.0729\n",
      "Epoch 2/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0724 - mse: 0.0724 - val_loss: 0.0727 - val_mse: 0.0727\n",
      "Epoch 3/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0722 - mse: 0.0722 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 4/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 5/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0727 - val_mse: 0.0727\n",
      "Epoch 6/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 7/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 8/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0725 - val_mse: 0.0725\n",
      "Epoch 9/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 10/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 11/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0724 - val_mse: 0.0724\n",
      "Epoch 12/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 13/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 14/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0725 - val_mse: 0.0725\n",
      "Epoch 15/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 16/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 17/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 18/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0726 - val_mse: 0.0726\n",
      "Epoch 19/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0724 - val_mse: 0.0724\n",
      "Epoch 20/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 21/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 22/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 23/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0724 - val_mse: 0.0724\n",
      "Epoch 24/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 25/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 26/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 27/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 28/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 29/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 30/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 31/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 32/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 33/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 34/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0725 - val_mse: 0.0725\n",
      "Epoch 35/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 36/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 37/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 38/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 39/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 40/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 41/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 42/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 43/150\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 44/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 45/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 46/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 47/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 48/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 49/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 50/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 51/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 52/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 53/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 54/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 55/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 56/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 57/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 58/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 59/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 60/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 61/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721: 0s - loss: 0.0713 - mse\n",
      "Epoch 62/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 63/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 64/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 65/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0725 - val_mse: 0.0725\n",
      "Epoch 66/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 67/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 68/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 69/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 70/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 71/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 72/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 73/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 74/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 75/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 76/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 77/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 78/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 79/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 80/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 81/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 82/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 83/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 84/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 85/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 86/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 87/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 88/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 89/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 90/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 91/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 92/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 93/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 94/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 95/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 96/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 97/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 98/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 99/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 100/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 101/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 102/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 103/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 104/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 105/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 106/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 107/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 108/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 109/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 110/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 111/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 112/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 113/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 114/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 115/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 116/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 117/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 118/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 119/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 120/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 121/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 122/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 123/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 124/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 125/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 126/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 127/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 128/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 129/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 130/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 131/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 132/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 133/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0724 - val_mse: 0.0724\n",
      "Epoch 134/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 135/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 136/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 137/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 138/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 139/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 140/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 141/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0724 - val_mse: 0.0724\n",
      "Epoch 142/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 143/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 144/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 145/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 146/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 147/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 148/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 149/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 150/150\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0723 - val_mse: 0.0723\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=150, batch_size=40,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35f91ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABAP0lEQVR4nO3dd3gc1dX48e/ZVZesZknuttwA94ptcEwzIaaa0GwHCBASAgkhhby/kDeNlzRIAUICISZACAFTTHMIJTHYtIBxwR0XuSFLtnrv5fz+uCNpV13GiwQ+n+fRo91pe3d2d87cc+/cEVXFGGOM6S5fbxfAGGPMp4sFDmOMMT1igcMYY0yPWOAwxhjTIxY4jDHG9IgFDmOMMT1igcOYEBKRv4nIL7q57H4ROfPjbseYULPAYYwxpkcscBhjjOkRCxzmmOeliP5HRDaLSIWIPCgiA0TkZREpE5GVIpIUsPwFIrJNRIpFZLWIjAuYN01ENnjrPQlEtXqt80Rko7fuf0Vk8hGW+WsikiEihSKyQkQGe9NFRO4SkVwRKRWRLSIy0Zt3johs98qWJSLfP6IdZo55FjiMcS4GPg8cB5wPvAz8L5CK+53cBCAixwHLgO94814C/ikiESISATwPPAokA09728VbdxrwEPB1oD/wF2CFiET2pKAicgbwa+AyYBBwAHjCm30WcIr3PhK8ZQq8eQ8CX1fVfsBE4PWevK4xTSxwGOP8UVVzVDULeAtYo6ofqGo18BwwzVtuEfAvVf2PqtYBvwOigZOBOUA4cLeq1qnqcmBtwGtcB/xFVdeoaoOqPgLUeOv1xOXAQ6q6QVVrgB8CJ4lIOlAH9ANOAERVP1TVQ956dcB4EYlX1SJV3dDD1zUGsMBhTJOcgMdV7TyP8x4Pxp3hA6CqjUAmMMSbl6XBI4ceCHg8ArjZS1MVi0gxMMxbrydal6EcV6sYoqqvA38C7gVyRWSpiMR7i14MnAMcEJE3ROSkHr6uMYAFDmN6KhsXAADXpoA7+GcBh4Ah3rQmwwMeZwK/VNXEgL8YVV32McsQi0t9ZQGo6j2qOgMYj0tZ/Y83fa2qLgTScCm1p3r4usYAFjiM6amngHNFZL6IhAM349JN/wXeBeqBm0QkXEQuAmYFrPsAcL2IzPYasWNF5FwR6dfDMiwDrhGRqV77yK9wqbX9InKit/1woAKoBhq9NpjLRSTBS7GVAo0fYz+YY5gFDmN6QFV3AlcAfwTycQ3p56tqrarWAhcBVwOFuPaQZwPWXQd8DZdKKgIyvGV7WoaVwE+AZ3C1nNHAYm92PC5AFeHSWQXAb715VwL7RaQUuB7XVmJMj4ndyMkYY0xPWI3DGGNMj1jgMMYY0yMWOIwxxvSIBQ5jjDE9EtbbBfgkpKSkaHp6em8XwxhjPlXWr1+fr6qpracfE4EjPT2ddevW9XYxjDHmU0VEDrQ33VJVxhhjesQChzHGmB6xwGGMMaZHjok2jvbU1dVx8OBBqqure7sonwlRUVEMHTqU8PDw3i6KMSbEQho4RGQB8AfAD/xVVW9vNT8S+DswAzemziJvoLbL8Ub09EwGpgN7cPdKaDIU+IeqfqenZTt48CD9+vUjPT2d4MFMTU+pKgUFBRw8eJCRI0f2dnGMMSEWslSViPhx9wQ4Gze88xIRGd9qsWuBIlUdA9wF3AGgqo+p6lRVnYobmG2fqm5U1bKm6d68AwQMItcT1dXV9O/f34LGUSAi9O/f32pvxhwjQtnGMQvIUNW93qihTwALWy2zEHjEe7wcmC9tj+RLaLktZjPvFp5pBNdAesSCxtFj+9KYY0coA8cQ3I1rmhz0prW7jKrWAyW4G9IEWoS7/0Bri4EntYPhfUXkOhFZJyLr8vLyjqD4kF9eQ3Fl7RGta4wxn1V9uleViMwGKlV1azuzF9N+QAFAVZeq6kxVnZma2ubCx24pLK+lpKruiNbtSnFxMffdd1+P1zvnnHMoLi4++gUyxphuCmXgyMLdUrPJUG9au8uISBiQgGskb9JucBCRKUCYqq4/mgVuQyBUtyvpKHDU19d3ut5LL71EYmJiaApljDHdEMrAsRYYKyIjRSQCFwRWtFpmBXCV9/gS4PWm1JOI+IDLaKd9A9fu0dP7NPdYKLP2t9xyC3v27GHq1KmceOKJzJs3jwsuuIDx413/gQsvvJAZM2YwYcIEli5d2rxeeno6+fn57N+/n3HjxvG1r32NCRMmcNZZZ1FVVRXCEhtjjBOy7riqWi8iNwKv4rrjPqSq20TkNmCdqq4AHgQeFZEM3K02Fwds4hQgU1X3trP5y4BzjlZZ/++f29ieXdpmelVdAwJEhft7vM3xg+P52fkTOpx/++23s3XrVjZu3Mjq1as599xz2bp1a3N31oceeojk5GSqqqo48cQTufjii+nfP7j5Z/fu3SxbtowHHniAyy67jGeeeYYrrriix2U1xpieCOl1HKr6EvBSq2k/DXhcDVzawbqrgTkdzBt19ErZN8yaNSvoGoh77rmH5557DoDMzEx2797dJnCMHDmSqVOnAjBjxgz279//SRXXGHMMO2avHA/UUc0gI7ccn8Co1LiQlyE2Nrb58erVq1m5ciXvvvsuMTExnHbaae1eIxEZGdn82O/3W6rKGPOJ6NO9qnpbKNs4+vXrR1lZWbvzSkpKSEpKIiYmhh07dvDee++FsCTGGNMzVuPojECIOlXRv39/5s6dy8SJE4mOjmbAgAHN8xYsWMD999/PuHHjOP7445kzp92MnTHG9Arp4Pq5z5SZM2dq6xs5ffjhh4wbN67T9fbmlaMKo9NCn6r6LOjOPjXGfHqIyHpVndl6uqWquvDZD6vGGNMzFjg64cZfstBhjDGBLHB0QgjdlePGGPNpZYGjCxY3jDEmmAWOTthI4cYY05YFji5YqsoYY4JZ4OhEX6pwxMW5LsHZ2dlccskl7S5z2mmn0brbcWt33303lZWVzc9tmHZjTE9Z4OiMCNrHWjkGDx7M8uXLj3j91oHDhmk3xvSUBY5OCISsdfyWW27h3nvvbX5+66238otf/IL58+czffp0Jk2axAsvvNBmvf379zNx4kQAqqqqWLx4MePGjeOLX/xi0FhVN9xwAzNnzmTChAn87Gc/A9zAidnZ2Zx++umcfvrpQMsw7QB33nknEydOZOLEidx9993Nr2fDtxtjAtmQIwAv3wKHt7SZnFrfQHKjQsQR7KaBk+Ds2zucvWjRIr7zne/wzW9+E4CnnnqKV199lZtuuon4+Hjy8/OZM2cOF1xwQYf38/7zn/9MTEwMH374IZs3b2b69OnN8375y1+SnJxMQ0MD8+fPZ/Pmzdx0003ceeedrFq1ipSUlKBtrV+/nocffpg1a9agqsyePZtTTz2VpKQkG77dGBPEahydCGUbx7Rp08jNzSU7O5tNmzaRlJTEwIED+d///V8mT57MmWeeSVZWFjk5OR1u480332w+gE+ePJnJkyc3z3vqqaeYPn0606ZNY9u2bWzfvr3T8rz99tt88YtfJDY2lri4OC666CLeeustwIZvN8YEsxoHdFgzyCuqpLSqnvGD40PyspdeeinLly/n8OHDLFq0iMcee4y8vDzWr19PeHg46enp7Q6n3pV9+/bxu9/9jrVr15KUlMTVV199RNtpYsO3G2MCWY2jE6HuVbVo0SKeeOIJli9fzqWXXkpJSQlpaWmEh4ezatUqDhw40On6p5xyCo8//jgAW7duZfPmzQCUlpYSGxtLQkICOTk5vPzyy83rdDSc+7x583j++eeprKykoqKC5557jnnz5h3Fd2uM+aywGkdnQtyrasKECZSVlTFkyBAGDRrE5Zdfzvnnn8+kSZOYOXMmJ5xwQqfr33DDDVxzzTWMGzeOcePGMWPGDACmTJnCtGnTOOGEExg2bBhz585tXue6665jwYIFDB48mFWrVjVPnz59OldffTWzZs0C4Ktf/SrTpk2ztJQxpg0bVr0T2cVVFFXUMmFIQiiL95lhw6ob89liw6ofoc9+WDXGmJ6xwNEJG6vKGGPaOqYDR3fSdFbj6J5jIeVpjHGO2cARFRVFQUFBpwc8QWyUw25QVQoKCoiKiurtohhjPgHHbK+qoUOHcvDgQfLy8jpcprSqjtLqesLKoj/Bkn06RUVFMXTo0N4uhjHmE3DMBo7w8HBGjhzZ6TJ/WLmbu1buYs+vzsHvswYPY4yBYzhV1R1+b+80NFq6yhhjmljg6ITPq2U0WjuHMcY0s8DRCb/XH9dqHMYY08ICRyea2jUarMZhjDHNQho4RGSBiOwUkQwRuaWd+ZEi8qQ3f42IpHvTLxeRjQF/jSIy1ZsXISJLRWSXiOwQkYtDVX6fV+NotBqHMcY0C1ngEBE/cC9wNjAeWCIi41stdi1QpKpjgLuAOwBU9TFVnaqqU4ErgX2qutFb50dArqoe5233jVC9h+YahwUOY4xpFsoaxywgQ1X3qmot8ASwsNUyC4FHvMfLgfnS9nZ3S7x1m3wF+DWAqjaqav5RL7nHZ6kqY4xpI5SBYwiQGfD8oDet3WVUtR4oAfq3WmYRsAxARBK9aT8XkQ0i8rSIDGjvxUXkOhFZJyLrOrvIrzP+5lTVEa1ujDGfSX26cVxEZgOVqrrVmxQGDAX+q6rTgXeB37W3rqouVdWZqjozNTX1iF6/+ToOq3EYY0yzUAaOLGBYwPOh3rR2lxGRMCABKAiYvxivtuEpACqBZ73nTwPTj16Rg1njuDHGtBXKwLEWGCsiI0UkAhcEVrRaZgVwlff4EuB19UYdFBEfcBkB7RvevH8Cp3mT5gPbQ/UGrHHcGGPaCtlYVapaLyI3Aq8CfuAhVd0mIrcB61R1BfAg8KiIZACFuODS5BQgU1X3ttr0D7x17gbygGtC9R7sOg5jjGkrpIMcqupLwEutpv004HE1cGkH664G5rQz/QAuqIScpaqMMaatPt043tusxmGMMW1Z4OiEz8aqMsaYNixwdKKpxmHXcRhjTAsLHJ2w6ziMMaYtCxydsFSVMca0ZYGjE367kZMxxrRhgaMTdiMnY4xpywJHJ5pvHWuBwxhjmlng6IRdx2GMMW1Z4OiENY4bY0xbFjg6YY3jxhjTlgWOTrQ0jvdyQYwxpg+xwNEJX9MFgJaqMsaYZhY4OmGpKmOMacsCRyfsOg5jjGnLAkcnfFbjMMaYNixwdMJqHMYY05YFjk7YPceNMaYtCxydsFSVMca0ZYGjE3YdhzHGtGWBoxM+u5GTMca0YYGjE001Dhsd1xhjWljg6IQ1jhtjTFsWODphjePGGNOWBY5O2HUcxhjTlgWOTtiNnIwxpi0LHJ3wWeO4Mca0YYGjEy2N471cEGOM6UMscHTCixuWqjLGmAAhDRwiskBEdopIhojc0s78SBF50pu/RkTSvemXi8jGgL9GEZnqzVvtbbNpXloIy49PLFVljDGBQhY4RMQP3AucDYwHlojI+FaLXQsUqeoY4C7gDgBVfUxVp6rqVOBKYJ+qbgxY7/Km+aqaG6r3AC5dZTUOY4xpEcoaxywgQ1X3qmot8ASwsNUyC4FHvMfLgfkiXot0iyXeur3CJ2I1DmOMCRDKwDEEyAx4ftCb1u4yqloPlAD9Wy2zCFjWatrDXprqJ+0EmqPK7xO7jsMYYwL06cZxEZkNVKrq1oDJl6vqJGCe93dlB+teJyLrRGRdXl7eEZfBL5aqMsaYQKEMHFnAsIDnQ71p7S4jImFAAlAQMH8xrWobqprl/S8DHselxNpQ1aWqOlNVZ6amph7xm/D5LFVljDGBQhk41gJjRWSkiETggsCKVsusAK7yHl8CvK7qTu9FxAdcRkD7hoiEiUiK9zgcOA/YSghZ47gxxgQLC9WGVbVeRG4EXgX8wEOquk1EbgPWqeoK4EHgURHJAApxwaXJKUCmqu4NmBYJvOoFDT+wEnggVO8BXOO4XQBojDEtQhY4AFT1JeClVtN+GvC4Gri0g3VXA3NaTasAZhz1gnbC77PrOIwxJlCfbhzvC6xx3Bhjglng6II1jhtjTDALHF2wxnFjjAlmgaMLfrELAI0xJpAFji74fGK3jjXGmAAWOLpgNQ5jjAlmgaMLPp9dx2GMMYEscHTB78NSVcYYE8ACRxcsVWWMMcEscHTBGseNMSaYBY4uWI3DGGOCWeDogs9u5GSMMUG6FThE5NsiEi/OgyKyQUTOCnXh+gK/WKrKGGMCdbfG8RVVLQXOApJwd927PWSl6kPs1rHGGBOsu4Gj6b7e5wCPquq2gGmfaT6f0GBxwxhjmnU3cKwXkX/jAserItIPOCYui/OL3Y/DGGMCdfdGTtcCU4G9qlopIsnANSErVR9iqSpjjAnW3RrHScBOVS0WkSuAHwMloStW3+GzxnFjjAnS3cDxZ6BSRKYANwN7gL+HrFR9iNU4jDEmWHcDR72qKrAQ+JOq3gv0C12x+g6f3cjJGGOCdLeNo0xEfojrhjtPRHxAeOiK1Xf4xW4da4wxgbpb41gE1OCu5zgMDAV+G7JS9SF261hjjAnWrcDhBYvHgAQROQ+oVtVjoo3DJ0LjMdHx2Bhjuqe7Q45cBrwPXApcBqwRkUtCWbC+wu/DGseNMSZAd9s4fgScqKq5ACKSCqwEloeqYH2FpaqMMSZYd9s4fE1Bw1PQg3U/1XzWOG6MMUG6W+N4RUReBZZ5zxcBL4WmSH2L1TiMMSZYtwKHqv6PiFwMzPUmLVXV50JXrL7DZzdyMsaYIN2tcaCqzwDPhLAsfZLfZ6kqY4wJ1GngEJEyoL2jpgCqqvEhKVUfYqkqY4wJ1mkDt6r2U9X4dv76dSdoiMgCEdkpIhkicks78yNF5Elv/hoRSfemXy4iGwP+GkVkaqt1V4jI1p693Z6z6ziMMSZYyHpGiYgfuBc4GxgPLBGR8a0WuxYoUtUxwF3AHQCq+piqTlXVqbhhTvap6saAbV8ElIeq7IH8PqzGYYwxAULZpXYWkKGqe1W1FngCN0hioIXAI97j5cB8EWl9Z8El3roAiEgc8D3gFyEpdSt+axw3xpggoQwcQ4DMgOcHvWntLqOq9bh7fPRvtcwiWroBA/wc+D1Q2dmLi8h1IrJORNbl5eX1vPQen8/FMWsgN8YYp09fxCcis4FKVd3qPZ8KjO5OV2BVXaqqM1V1Zmpq6hGXwe9VgCxdZYwxTigDRxYwLOD5UG9au8uISBiQgLsqvcligmsbJwEzRWQ/8DZwnIisPqqlbqWpxmHpKmOMcUIZONYCY0VkpIhE4ILAilbLrACu8h5fArzu3TAK754flxHQvqGqf1bVwaqaDnwO2KWqp4XwPeBvSlVZjcMYY4AeXADYU6paLyI3Aq8CfuAhVd0mIrcB61R1BfAg8KiIZACFuODS5BQgU1X3hqqM3dGcqrIahzHGACEMHACq+hKtxrRS1Z8GPK7GDdXe3rqrgTmdbHs/MPFolLMzLY3joX4lY4z5dOjTjeN9gd/rHGyN48YY41jg6ILfGseNMSaIBY4u+Kxx3Bhjgljg6II1jhtjTDALHF2w6ziMMSaYBY4uNNU4LFVljDGOBY4uWOO4McYEs8DRBWscN8aYYBY4utDSON7LBTHGmD7CAkcX/N4eslSVMcY4Fji64LPGcWOMCWKBowvWOG6MMcEscHSh+ToOq3EYYwxggaNLzddxWI3DGGMACxxdslSVMcYEs8DRBZ/dc9wYY4JY4OiC327kZIwxQSxwdKH5Og6rcRhjDGCBo0s+axw3xpggFji6YI3jxhgTzAJHF6xx3Bhjglng6EJL47gFDmOMAQscXfLblePGGBPEAkcXfHbPcWOMCWKBowt+u5GTMcYEscDRBbuRkzHGBLPA0QWft4escdwYYxwLHF2wxnFjjAlmgaMLfmscN8aYICENHCKyQER2ikiGiNzSzvxIEXnSm79GRNK96ZeLyMaAv0YRmerNe0VENonINhG5X0T8oXwPPmscN8aYICELHN4B/V7gbGA8sERExrda7FqgSFXHAHcBdwCo6mOqOlVVpwJXAvtUdaO3zmWqOgWYCKQCl4bqPYDVOIwxprVQ1jhmARmquldVa4EngIWtllkIPOI9Xg7MF/GO1C2WeOsCoKql3sMwIAII6RHdZ2NVGWNMkFAGjiFAZsDzg960dpdR1XqgBOjfaplFwLLACSLyKpALlOECThsicp2IrBORdXl5eUf6HvDihqWqjDHG06cbx0VkNlCpqlsDp6vqF4BBQCRwRnvrqupSVZ2pqjNTU1OPuAwto+Me8SaMMeYzJZSBIwsYFvB8qDet3WVEJAxIAAoC5i+mVW2jiapWAy/QNv119LxzD2FbngSsxmGMMU1CGTjWAmNFZKSIROCCwIpWy6wArvIeXwK8ruqO0CLiAy4joH1DROJEZJD3OAw4F9gRsnew7TnCNv0DsDYOY4xpErLA4bVZ3Ai8CnwIPKWq20TkNhG5wFvsQaC/iGQA3wMCu+yeAmSq6t6AabHAChHZDGzEtXPcH6r3wLBZSPYHhFFvgcMYYzxhody4qr4EvNRq2k8DHlfTQXdaVV0NzGk1LQc48agXtCNDT0TW3M8J8hGNesIn9rLGGNOX9enG8V43bBYAM/x7rMZhjDEeCxydSRgGcQOZ7tttY1UZY4zHAkdnRGDYiUyTXTY6rjHGeCxwdGXoLIZLLpE1hb1dEmOM6RMscHTFa+cYUr6llwtijDF9gwWOrgyaSh1+hpRv7XpZY4w5Bljg6Ep4FBmMYFDlzt4uiTHG9AkWOLrhkKQSX5fbdsaGR+G1n3/yBTLGmF5kgaMbciWFhLp2Rtjd8hRseKTtdGOM+QyzwNEN+b4UohorobokeEZJFlTkQX1NaAtQWwH3z4MD/w3t6xhjTDdY4OiGfJ93i5DS7JaJqlDqDfZbdii0BcjbAYc3w7bnQ/s6xhjTDRY4uqHAl+IelAaMCl9ZCPXV3vTstisdTYX73P/MNaF9nc6U58H6R1zANMYc0yxwdEO+vylwBASIkoCbG5a0vs3IUVa03/0/vMWlrXrD+ofhnze1lMUYc8yywNENxb7+NCLBgSOw9lEa6sDh1Ti0AbLWh/a1OpLjXcdSkNE7r2+M6TMscHSD+iMo9ScFB4imWob42qaq1j8CS0/rflonZzv8dgwU7Gl/ftEBSPWGde+tdFXONvc/f1fvvH5vy98NjQ29XQpj+gQLHN3gE6HIn9KqxnEQ/BHQf2zbGsfuf0P2B1C4l2458I7rnZXxWvvzC/fBoKmQOg4y3z+i9/Cx1Fa2vJf83Z/86/e20kNw72xY91Bvl8SYPsECRzeE+4UCf0pwW0bJQYgfDAlD2waO3O3uf9aG4Ollh+G9P0NjY6vlP3T/P3q37YvX17jtJ6W7cbMy17RdP9TydoA2AnJsBo7Dm12acMe/erskxvQJFji6YWRKLPtqElo1jmdB/FBIGBI8vbaipRdU1rrgDb2/FF65BbY/Hzw9z7tt+kfvtU1vFX8EKCSPhOFz3LUkn3S6qClNNfwkKDgGA0fT+z/wDtSU925ZjOkDLHB0w6ShieypSYCaEqgpcxNLs1zQiB8C5blQX+um5+0AFMTftiF7z+vu/xu/aak1qLoaR1g0lGUH99aCll5MSekwbLZ7fOCdjgu7fQUcPsoDMuZsg/AYGPt5KM9peyHkZ13ONteW1VAL+97o7dKYvqyxEbY+23I8OBLZH/T53osWOLph8tAEsrXpIsBDrpG0NNulqeIHA9pyEWCOl6Y6bgEc2tzyBaoshOyNMHAS5H0IH77gplfkQVUhTLzIPf+oVeN3U+0laSQkj4L+Y9wXsz2VhbD8K/Da/x2Nt90idxukjYPU493z/M9Qz6ru/MBzt8Oo0yGiH+x6NfRlMp9ee1fB8muOfCiixgb4x8Xw4veObrmOMgsc3TBhcDw5JLsnpVnurFsbXG0jfrA33UtX5W53tYeJF0FDjTvoAuxdDSic8ztIOa6l1tHUvjHxIndgat3OUbTfne3Hpbk7Ek5eDAfe9lJYrWx7Dhrr3NAkDXVH582ruhrMgAmu3HD0UmUfPNZxT7KjoTgTNj3Z8fyMlfDroZ2329TXuvc7aAqMPg12/+ezeRGkKux4KbSfR0cK9sBjl7k2wECNje4z/DTt76asQkcnd13J2gCVBbD/bdcp5eMoO+xqLyFggaMbYiLCiOo/3D0pzXYN4+DVOIZ6070G8pxtkHZC8w2gOOi1c+xdBZEJMGQmzPu+CzD734Q8b7j2tAkw7MS23W2L9rk0lYh7Pvky93/zU20LuuVp8IVBbfnRu96jPMfViAZMdOXwhR15O0fpoZYUXdF+eOEb8NgloUt9vfkbeO66jnu3vXufC+7bX+h4G/m7oLHeBc6xX3DpxKY2j8+KmnJ49mvwxBL423lQltO99RoboaG+5Xl5LuzvJI3a0TZeuBF2vwpbn/HKUwZPfRnuGAF3T4RNT/Rsm0fLkQSsvavd/4/+e2QXBmf8x/1vqHHB4+NYcz88cEbbgHwUWODopkFD0wHQ0qyWwBFU4/C+JLnbXRBIGAaxqe4MQhX2rIKR88AfBuMvgMh4dzac9yFEJUC/gTBsjjsoVRW3vHDRfnfAbpI0AoafDJufDP5iF+13tZU5NwACe7uZi1//N3j+my1nN4e3Bh9Imy78SxsP/nCXMjuSGkfhXrh7kvsygzu7BXeNygs3Hv2zysZG2PlK8GsFlWcf7PG6P+98uePtNPWQSxvv2ngAdrzY/XK8+D24Z5r7Aa/vgyMp19fCwwvcQXv29VBd7A7a3Unhvfht+Mu8ltrtM1+Fv50DGx/v/uuvf9gdZMOiWj6HLcvdd3D8BZA4vG3ap6Eeln2p5fMNhTVL4Y8zXPo30N434A9TW44Bgcpz3e9l2pXu+bbnev66GStd7TY8piWIHIm6Kvd9O/4cd2w5yixwdNP44WnkazyVeR+1BImEoRAV71JMpdluPKeKPBgw3tUQhsx0P4rd/3aN3qNPd+uFR7sfxYcrXGBJHeeWT58LqDuYgzuYFu13B+tAUxa5g/feVS3Ttjzt/s+6zn3xutOIW3oIXr4FNv4DHr/MfdH+Ot8dOA5tdsvse8v9HzDB/U8Ze2RtHGsfdGm0tX/10iIvugB75q1uPzx5hTtY9KRRcctyeOv37c/LWg8Vua6Tws52Asf6v7l5M7/ier91dJadsw184e599xsII09xB8budIkuznTXfkQlurP6l/8fVOR39911rroEdv3743fNXvegG8rmkofh7Dtg4b2Q+R6svLXz9UoPuf2Qux0+eNT1CNz3hjtZeuHG7nVdLs2G//wMRp0Gc77hUqyVhe6kKOV4uOBPMOMad0IUmELb+S/3t/LWlhOOg+uO3pl1ZSG8/nMo3ONSyoHeu89lAd64wz2vr3GfQ0N9S23jxGvddVdNNahAdVXwzj3t/4Yq8t3x4ITzIH2eS4seqS3LXaZg9tePfBudsMDRTZOGJHBYk9EDb7uzjog4V1MAV+sozWppz0gb7/6PnOcO/I976aVRp7dscPJil1I6tNGltgBGzIVx58Nrt8GBd90ZcV2l64obaPyFEJ0Ej34R/jQLHjrbfRlHzHVnaKNOdRcKdjWu1apfujTMmbe6H+0/b4IhM1xKbfWv3VnVmr/AhIsgxmvjSRnrflCtr6Kur3VnS4Gpiya1Fe7gEpPi1t3+gjsYnHAunPwt+Nx33fOnvgyPXdy94FF0AF74pttX7V0UufNfLq124lfdtisKAspaAx/8A44/G2Ze66bt6uDsNWeb6xTgD3fPp14OxQfav+amtY2Puf+X/g0WPeoGxVz7YNfrdUbVpSn/OBMevxTeufvIt1VV5A6Ao06D8QvdtIkXuWC65v7OU3LrH3bfgdRxsPp2d0OzmBS44V0YPA2WX9v+WXmg1be7lMx5d7nvgja4/fPRu+7kSASmLHY92gLTVe8/4IJ+3ofuN3JoMzx4Fjz0heDP+Ui9fadLl406HdY+0HKQLz3kTgKjEl37XP5ueO569zn85yeufSM6GQZOgYkXQ/aG4ICX+yEsPd0tu2xx29/nntcBhTHzYcyZLkAFrl+e52o1gWrKXABf91DL70YV3v+LOw6lz/v4+6MdFji6adygeB5sPA9/ZYGrQiYMbWl3SBjielNt9s76mwLHnG/A9e/AxQ+6M7r+o1s2OGKuS2eB+/GB297Ce1066h8Xu7/E4a6HVqDoRPjGe3D2b13Q8vldsDjjx27+yFPd2X17B7fGBne2mrnWHdhmXecO3EuegNN/DF9+AU6+0Z2lP321u/Dv8wG9tAZOdt1S376zZVp1iTvg/+Nil75onXba8rRb5qK/uKD04nfcdsed597zmbfCzbvcAWTfmy4gNDa4DgCl2W571SUuNdHU6+yVH7oDSmwq/OenbV9zx0sw4mSYusS9VlNgOPCua1epzHcHyAETIGF4x4Ejd3vL5wkusEf0awkK5Xnu5KCpm3bgfv7gH66WmTTCBZ+xX3DX8tRVubPULcvbf83OrLzVtUckDoOxZ7kz4/baFeprXK2qqRNF4T549usttSVVWH2HS4ue9YuW7zLAGT+ByH7w8g/aTyHW17gD1XFfgPP/4NrBDrwNc2+CuFS49GG3zzu7O2bBHrd/ZlzjegsOng6xaS1n8pO8k634wS6wbXrC60yyA/a/Baf+AOIGwjt/cN+X6ER3YH/yiu7fHydnG/z7x66GtHe1O/Ae3urSVFOWwEVLXQrt3z92+2HT4+59LXkCwiLh4XNg27PuN/Hefa5BfNSp4PPBpEtcJ5nlX3HfjR0vuaBRme/2b0GGq4FufQbumgR//hz8948u+A6aBmPPdGXc/KTbV6t+5VK9985ytau6avj3T+B3x8HzN8CL34X7Pwcbl7la+OEt7rcd+LkeRWEh2epnUFS4n11pCziz5CR+OX4vsyZPIKZpZuo4d7ZQuMcd6OPS3HQRGDjR/bXm88GkS90BuKnGAa4Wc+kjrkvfxIvdQT08uu36/QbC7OvcX2vDT3LDofznVveFbahxB5Dij9xZYKNXK4hMgFO+7x4fd5b7A5frfu8+OLgW5t3s3lOTCV90Z12v/8L90GL6uwNUwW44/lx3MIgbCNO/7M6oKvLgvftd4/ro+e5M8v2l7mA9cHLLdv1h7kDelCbY/oIrN0B4LNRXeVev41JxhzbBmf/nUoUvftelRkad6uaX50L+Tre9QVNdW9QHj7oLL3f/2wWbBbfD6DPcZ3T8Ancb4PzdrrtzRZ5LGWStdzXJAQGBIyIWJlzoDhKJw+HN37kgDe4Ad97droa4d5VLT54VcPA8+VvwyHmuvaOp7eTAO3D2b1pqNJ1Z95CrYcy4Bs79vauNLj3N1dRGnerOdqdf6YbBefIKdzYeFuUOYtued8tvfgLevRdqSt33YdoVrot4oJhkdxLy0vfd2f3Ma1zt+IPHXLfz2nK3j2ZfD8Nnu9RK5pqW2lvicDjpm+67Pfvr7vPa87p73UOb3Hcua707+M672a3j83mfw9/dWXLisJbyTL0cnrnW1YKLP3Lf7ROvdd+Z125zyyz6hzuhWf4Vd9OzCV90NZ/IOKgudb9NxH2+RftdEDi0ydVKw2Pc96P5uxgJp//Q/Y5P/X/uxOT5G9x7HDEXRpzk3vvbd7rv2Nm/cbX//W+1ZBXiB8Nlj8CyJa42lLfD7YclT0K/Ae638fad7vcyaIor26GNMPUKty+SR3m9L+9oCabjL3TLPHK+O+nM3+kyFzO/4tqm/vV9eP56t2zcwJaONCEg+mnq6naEZs6cqevWret6wS5syizml//6kPf3FzJuUDwrbpxLuN/H5o8K2Lx1M2PS4hidnk5qSipVtQ08vT6T/fmV3HL2CUSEtVTuDhRU8Oi7B/juSQnErv0TzP8ZhEd97PIFee3nrh2h7LD7oSWNgMQR7kcdm+J+tCPmuusz2rPh77DuYbhqhTv7DNRQ737ITVfAxw+BC+9zNZ0VN7ofQ2sX/RUmX+rO6O6fC7NvgLNvb7ucqkuPFe1zZ+na6FIFkf1c6i9nG7x1pzv4X7faHfjvmxM8aq8vzAXHb2927/tfN7u2lch4d9A68WsQEdOy/Edr4OGzXaokMt4dVMHVaNImwMV/DQ7uH73n0iLgDlBjznQHtHfvc687cp4rT3UJfO9Dt6+b3ttf57v3cOoP3Px37nYH/MZ693opY10wLjvk2kWik1ywqq92Z5pj5sPiZe6gCa6m++J33YG87DDUVbjPozTbBa3sjbB1uWubWXivK/tbd7pa8oSLvDPjyLafQ0M9PHSWO8BHJ7vXr6t0gai+2p0IXP+22/91Va6scakt61eXwh+nu8+izrt7Zr9BkDza1U7AnRSdeWvLOjtfgWWLXDmnXdEyva7KpXv3vemeT17saq+VhXDPVHdCcunDbt6W5S7AHvgv0MmxbdAUmPIl9/4j4txvJX+X23fDZrd83qqunWP1r9zzC+93tdi6apd5OO5s91lUFroaw+e+05LCBlcDeP56GPN5l7KMjPP2b52rcaSOc0HQ53ff87g0dzIE7juVvdF9H9PGuTRyWY6rMZfnuP3U1GGjaT8VZLjvTGwahEV0/P67SUTWq+rMNtMtcPTcCxuz+PYTG/nZ+eM5f8pgFtz9FvnlLdXjtH6R1DU0UlTpzkQvmjaE3182BRFBVfnSA2t4d28BV8wZzi8unNRm+3llNfzx9d18+aR0xqTFHZUyP7vhII0Kl8wYelS2R0O96yOeMNTVfpqqxA11XiN3taspxaa5ZQLbaXa85H6csf2P7LXra11AaQq2hza71Fp4DO5izBx3Vjf3225+yUFX5Z9+lQua7SnY4xp3D212Z6VDpruDS0Rs22VVXfvQwEktbQPgul+uvNWdCTbUuZpB6xphZaE7M27q6bLteVcLiox3Naz83S591G+gC5bVxe7sNCzKBcEv/Lrl4NNadYkbC23TMncy0nRRaWm2OwP19TAzXV/rai3bnnOf5YlfdQGjPNftl47K0WTLcteOMXyOC64nnOsCydZn3HfkgnvcQa6JqmsQHjPfHUhby9/tUooTLnKBD1ybRnRS2/dWke8OvDVlrqzJo9z0ggy3rwNPBLpjy3J3ovTFpcEnHd1RtN/VENp7T0eiod6d5LQX8I8yCxxHMXCoKl9+6H02ZhYzZWgi7+8vZNnXZlPXoGzLLmVbdgn1DcoVc0bw7p4C7lq5i2+dMYbvff44XtpymG8+voHjB/RjZ04Zj391NiePaTmYFVfWsnjpe+w4XEZKXASPfXUOxw90Z/yZhZWsO1BISlwkk4YkkBjTvTOKAwUVnHnnG9Q1KPcsmcYFUwY3v49l72dSVFnLN04bjYQoH2qM+XTqKHCEtI1DRBYAfwD8wF9V9fZW8yOBvwMzgAJgkaruF5HLgf8JWHQyMB3YBTwNjAYagH+q6i2hfA/tERF+dv4EFtz9Jm9n5POz88czY4TrdTRnVPBZ9InpSXxUWMkfX8/grd355JRWM25QPMuvP4nz//g2Nz+9iatOTie9fwx55bU8tTaTvfkV/PqiSdy9cheX/eVdRqXGcqi4msOl1UHb/vnCCVx5Ujqqyu2v7GBsWr/mGsX27FISY8IZnBjNb1/dSZjPx4TB/fj+05sAGJ4cw/2r9/DKNteFsX9sBItnDccYY7oSshqHiPhxB/rPAweBtcASVd0esMw3gMmqer2ILAa+qKqLWm1nEvC8qo4WkRhgtqquEpEI4DXgV6rayRVcR7/G0eThd/axJ6+cny+c2OnZekOj8sz6g9z5n10cLq3mqa+fxKyRyWzKLOZbyz7go8KWoQXiIsO4e9FUzhw/gP35Ffzkha2ouvTXlGGJzBqZTGFFLX96PYNNB4t57eZTWbUjj/99bgsAi2YOo75ReWbDQWIi/Fw5ZwR/eXMvN50xhqvnjuSi+95hf4F7vTCf8IMFJ/Dm7jze31fIY1+dTZjfx+GSKqrrGlGUhOhwhifHNqfMCitqeW9vAV+YMBC/79NXQ1FVq1kZ002feKpKRE4CblXVL3jPfwigqr8OWOZVb5l3RSQMOAykakChRORXbjX9UTuv8Qdgq6o+0FlZQhU4eqqqtoHMokqOGxDc2FxSWUdmUSWp/SJJjYvE140DcmZhJWfe+QYnpiezMbOYyUMTmDY8kXtX7SHcL3xl7ki2Hyrlrd35pMRFsPp/TicuMoyy6jq2ZZdSXl1PekoMY9L6kV9ewzl/eIvcso67MV4+ezhnjh/AD5/ZwuHSak4/PpU/LJlGfJTrDfTW7jx+88pO0lNimTc2hQumDCYq3OV0DxZVsimzhAOFFZw/eTDDkmMoqqjlG49tYExaHDfNH0tqv5Z8bW5ZNTERYcRFdl4hzi2rpq5BGZLYTq+zVgorarntn9t4c3c+z33jZEb0b6ftwhgTpDcCxyXAAlX9qvf8Slxt4caAZbZ6yxz0nu/xlskPWGYPsFBVt7bafiKwAThTVdsMRiQi1wHXAQwfPnzGgQMHjvI77H1/fG03v//PLmIi/Lz6nVMYlhzD+gOF9I+NJD0lFlVlxaZsBidGc2J6cqfb2pNXzqoduQxPjmFIUjTR3kG/tLqeFzdl89A7+2hUSO8fw4XThvCn1zMYlhzDtZ8bSUSYjx89t4UB8VHU1DeSV1bD8OQYvnXGGF7fkcvLW1uu6B2UEMUjX5nFj5/bygeZRahCRJiPk0f3Z3BiNFuySvjgo2Kiwn2cM2kQk4YkUFvfyJCkaE47Po3IMB+bMotZ9n4mKzZl0dCoLDpxGHNG9efZDVnszS/nuLR+zEhP4kuzhhMfFc5zH2Tx65c/pLiyDr9POOW4VB748kw2fFTEHS/vYOKQBGaMSKKwopb88hqmD09iZnoS/80oYPWuXL58UnpQsD9YVMny9a5GN35QAuMHx5McG9zeVFPfQGMjREe4/dj0O+tubae2vpGy6jr6x4W+AdSYjnwqA4eIzMa1jUxqte0w4J/Aq6p6d1dl6Ss1jqOtpr6Bbz3+AedOHsTCqUNC+lobM4t5Y2ce184bSVxkGO/uKeAnL2wlI9fd2GhWejIPXDWT+Kgw3s7I59YV29iTV0FcZBjXzE3nrPEDaVTlmr+tpaSqjoZG11A/cXA8f1qVwfbsUrKKqhiWHMM5kwaSXVLNPzdmU1bTciV6RJiPcJ9QUdtAVLiPRTOH4fMJ/3jvAHUNyuCEKKYNT2J3bhm7csqJiwxjSGI0O3PKmDIskdsvmsSqnbn85pWd/PS88dzzuhussbK2gdr6jofuSIwJ52/XzCLMJyx9cy//2nKIRtWga+MGJUQxLDmGlLgIsoqr2ZZVQn2jEhXuwy9CZV0DKXGRzB3dnzFpcYgI/aLCGJMWR3l1Pa9sPUx+RS1nHJ+K3+/jz6syyC6p5sxxA7hg6mAyCys5WFRFhF9IiYvk8jkjgoKVqpJfXktKXISl4sxR86lMVYnIXUCeqv6q1bYfAspV9abulOWzGjh6m6qy/VApGz4q5tIZQ5tTU+DOmN/dW8CUocG9v3YcLuX6R9fzpdnDue6U0e1ttllNfQNVtQ2E+31syy7lla2HqW1o4OTRKcwdnUJCjEuTZRZWcqikmhkjkprbXXYcLuWPr2ewJ7ecr586ioVThuDzCTX1DZx115scKHCpwWeuP5m0+Eh25ZSR2i+ShOhw3t1TwNr9RcwckcTI1Fiufvh9Dpe4tFhcZBhLZg3j6rkjiQn3s/1QKduzS9l+qJSs4iryy2pI6RfJ9OFJJESHU1hRQ6NCTISfjworeSejIKjrdpOE6HD6x0awN98NQzFteCKz0pN5cl0mxV637v6xEdQ3KqXVdfSLDONr80YRHeEns7CSlR/mklVcxYj+MZx6XCqHSqrZnVPGwIQoRqfGsb+ggq1ZpcRG+BmSFM0Vc0ZwwZTBFFTUcu+qDAbER3HlnBFEhPl4f18hsZFhTB6SgAgcLKoiJsJvtZ9jUG8EjjBc4/h8IAvXOP4lVd0WsMw3gUkBjeMXqepl3jwfkAnMC0xFicgvgHHApara8WliAAscJtA7Gfn8/MXt/P6yKUwYnNDl8jml1dy6YhvThiey2Et/HSlVpbahEUEorKhld24ZPhFmjUwm3O8jI7ec0uo6pg1LRESorK1nd045I1Njm193V04Zt/1zO29nuIxuRJiPz41JYcaIJNbsK+S9PQUMTY7m+AH9yC6pZm9uOcP7xzB5aCI19Q1szSphV045Z5yQxuaDxRRVuhpgcmwEjarNgWpgfBQicKikmjCfMH9cGvNPGMCAhCgiw3yUVNUREeZjwuB4MnLL+fVLO9hfUMFVJ6Vz7uRB7MopI7u4msGJUSTGRJBTWk1dQyNnTxxEcmwEDY1KdnEVgxOj8fuErVklvL4jlzFpcUwemsDunHI2fFREXlkNpdV1RIeH0T8ugnlj3YmDzyfUNTTiE8HvE/LLa1h/oIisoiqKKmsZkhjNWRMGtkkjBmpsVBpVCfP3fPSlmvoGIsM6vjbjo4JKBiREdrpMX9cr13GIyDnA3bjuuA+p6i9F5DZgnaquEJEo4FFgGlAILG4KEiJyGnC7qs4J2N5QXDDZATSdtv1JVf/aWTkscJjPGlXlcGk1MeFh9IsKC+pQ0VXPsYZG5f439nD3yl2MSevH3YumUlFbz9I39hId4efsiQMpr6ln5Yc5iAizRyaTVVTF8vUHKajoeADKIYnRjBsUz2s7cjodJT8yzMdJo/uz+WAJhRW1xET4GRAfxb78toNyhvmE5NgI+kWFUVXbQH5FLbX1jQxNiibC7+NAYSWNql7Hj7YDbPp9wqiUWJJiIkBcR5TIcB9jUuOorm/gv3sKqG9Qzps8iKnDEtlxuIys4ioaG5WoCD/jB8UzcUgCEwfHkxgTwa6cMt7enc/zG7PYll3KjBFJLJgwkLT4SJJiIpg7JgW/T3h2w0G+99QmYiP8zBubyuDEaKLCfaTERTIwIYqBCVEMSoiif2wkfp/w8tZDPPzOfiL8Ps44IY3jBvYjJsJPdLif6Kb/4X4SosODPuu6hkbW7i/kQEElCdHh5JfX8PKWw+SWVbNk1nAWnTiMfh/jRMcuALTAYUyQvLIaEmPCCe/m2XZ9QyOHStz1RHX1jcRHh1NeU8+27FIiwnzN6cpdOWVsOVjC+MHxDE2K5nBJNcVVdQzoF0VFbT1/f/cA72TkM2NEEtOHJ5KRW87+gkpOPz6V86cMZn9BJVuzShibFse04UnNHQwAqusaeHXbYZ7/IIuocD+jU+PwCRRX1TE4MZqZI5IYkxZHfFQ42w+59OaevPLmgJcYHU5VXQO7c8rxCZw8JgVVeGnLIarqGogO9zM8OYYwv1BaXUdmYVXza0eE+ZrbwiYPTWBWejJvZ+Sz43DLAJez0pO5fM5wvv/0JqYNS2J0Whxv7sqjpKqOqroGGhrbHm+btjsqJZZwv4+dOWVtlmkyID6SJbOGMzw5htd25PLmzrygdkCAMWlxJEaHs+5AEf0iw1h586kMiD+yIY0scFjgMMZ0oLymntzSakb0jw26Pqmkqo7t2aVszSrhcGk1E4fEM3NEMsOSW4YdyS2rpqy6nnX7C7ntn9upqG1gdGosz94wt7kdDlxNsKiyjsMl1RwureJwSQ2FFTUUV9YxbXgSCya6a6Oyi6s4VFJNVW0DVXUNVNbWU13XQEVNA2/uzuONXXmoQmq/SM44Po0zxqUxYXA85TX1RIX5SU9xXc03Hyzm9R25fOfM4454v1jgsMBhjAmxAwUVPPzOfq793Mig4HI0ZRZWUlJVx/hB8d265uvj6JUhR4wx5lgyon8st14wIaSvMSw5hmFdLxZSdiMnY4wxPWKBwxhjTI9Y4DDGGNMjFjiMMcb0iAUOY4wxPWKBwxhjTI9Y4DDGGNMjFjiMMcb0yDFx5biI5AFHeienFCC/y6V6l5Xx4+vr5QMr49FiZey+Eaqa2nriMRE4Pg4RWdfeJfd9iZXx4+vr5QMr49FiZfz4LFVljDGmRyxwGGOM6RELHF1b2tsF6AYr48fX18sHVsajxcr4MVkbhzHGmB6xGocxxpgescBhjDGmRyxwdEBEFojIThHJEJFbers8ACIyTERWich2EdkmIt/2pieLyH9EZLf3P6kPlNUvIh+IyIve85Eissbbn0+KSEQvly9RRJaLyA4R+VBETupr+1FEvut9zltFZJmIRPX2fhSRh0QkV0S2Bkxrd7+Jc49X1s0iMr0Xy/hb77PeLCLPiUhiwLwfemXcKSJf6K0yBsy7WURURFK8572yHztjgaMdIuIH7gXOBsYDS0RkfO+WCoB64GZVHQ/MAb7plesW4DVVHQu85j3vbd8GPgx4fgdwl6qOAYqAa3ulVC3+ALyiqicAU3Bl7TP7UUSGADcBM1V1IuAHFtP7+/FvwIJW0zrab2cDY72/64A/92IZ/wNMVNXJwC7ghwDe72cxMMFb5z7v998bZUREhgFnAR8FTO6t/dghCxztmwVkqOpeVa0FngAW9nKZUNVDqrrBe1yGO9gNwZXtEW+xR4ALe6WAHhEZCpwL/NV7LsAZwHJvkV4to4gkAKcADwKoaq2qFtPH9iPu1s7RIhIGxACH6OX9qKpvAoWtJne03xYCf1fnPSBRRAb1RhlV9d+qWu89fQ8YGlDGJ1S1RlX3ARm43/8nXkbPXcD/AwJ7LfXKfuyMBY72DQEyA54f9Kb1GSKSDkwD1gADVPWQN+swMKC3yuW5G/flb/Se9weKA364vb0/RwJ5wMNeOu2vIhJLH9qPqpoF/A535nkIKAHW07f2Y5OO9ltf/R19BXjZe9xnyigiC4EsVd3UalafKWMTCxyfQiISBzwDfEdVSwPnqetf3Wt9rEXkPCBXVdf3Vhm6IQyYDvxZVacBFbRKS/WB/ZiEO9McCQwGYmkntdHX9PZ+64qI/AiX8n2st8sSSERigP8FftrbZekOCxztywKGBTwf6k3rdSISjgsaj6nqs97knKaqq/c/t7fKB8wFLhCR/bgU3xm49oREL+UCvb8/DwIHVXWN93w5LpD0pf14JrBPVfNUtQ54Frdv+9J+bNLRfutTvyMRuRo4D7hcWy5g6ytlHI07Sdjk/XaGAhtEZCB9p4zNLHC0by0w1uvBEoFrPFvRy2Vqait4EPhQVe8MmLUCuMp7fBXwwiddtiaq+kNVHaqq6bj99rqqXg6sAi7xFuvtMh4GMkXkeG/SfGA7fWg/4lJUc0Qkxvvcm8rYZ/ZjgI722wrgy16voDlASUBK6xMlIgtw6dMLVLUyYNYKYLGIRIrISFwD9PufdPlUdYuqpqlquvfbOQhM976rfWY/NlNV+2vnDzgH1/tiD/Cj3i6PV6bP4dIAm4GN3t85uDaE14DdwEogubfL6pX3NOBF7/Eo3A8yA3gaiOzlsk0F1nn78nkgqa/tR+D/gB3AVuBRILK39yOwDNfmUoc7uF3b0X4DBNc7cQ+wBddDrLfKmIFrJ2j63dwfsPyPvDLuBM7urTK2mr8fSOnN/djZnw05YowxpkcsVWWMMaZHLHAYY4zpEQscxhhjesQChzHGmB6xwGGMMaZHLHAY04eJyGnijTBsTF9hgcMYY0yPWOAw5igQkStE5H0R2SgifxF3P5JyEbnLu6fGayKS6i07VUTeC7g3RNP9K8aIyEoR2SQiG0RktLf5OGm5d8hj3pXkxvQaCxzGfEwiMg5YBMxV1alAA3A5bmDCdao6AXgD+Jm3yt+BH6i7N8SWgOmPAfeq6hTgZNyVxeBGQf4O7t4wo3BjVhnTa8K6XsQY04X5wAxgrVcZiMYN9NcIPOkt8w/gWe9eIImq+oY3/RHgaRHpBwxR1ecAVLUawNve+6p60Hu+EUgH3g75uzKmAxY4jPn4BHhEVX8YNFHkJ62WO9LxfWoCHjdgv1vTyyxVZczH9xpwiYikQfM9uEfgfl9NI9l+CXhbVUuAIhGZ502/EnhD3R0dD4rIhd42Ir17NBjT59iZizEfk6puF5EfA/8WER9uxNNv4m4QNcubl4trBwE39Pj9XmDYC1zjTb8S+IuI3OZt49JP8G0Y0202Oq4xISIi5aoa19vlMOZos1SVMcaYHrEahzHGmB6xGocxxpgescBhjDGmRyxwGGOM6RELHMYYY3rEAocxxpge+f8FYXPQIEoo3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd92fad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=[0.27789945 0.32741291 0.44299102 0.37546384 0.29763233 0.31981987\n",
      " 0.34224106 0.32941739 0.33554306 0.44806013], Predicted=[28.80546]\n"
     ]
    }
   ],
   "source": [
    "Xnew = X_test\n",
    "ynew= model.predict(Xnew)\n",
    "#invert normalize\n",
    "ynew = scaler_y.inverse_transform(ynew) \n",
    "Xnew = scaler_x.inverse_transform(Xnew)\n",
    "print(\"X=%s, Predicted=%s\" % (Xnew[0], ynew[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d62a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = scaler_y.inverse_transform(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7ecf150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8697, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74435f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0008369898459461567"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, ynew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ab4e4",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "The precision of our learning model is so not good, we could improve it, for example, if we did a broader study about the structure of the data, such as calculating the correlation between the variables and from there doing data engineering to improve our neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
